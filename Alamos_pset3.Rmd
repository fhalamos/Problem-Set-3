---
title: 'Unsupervised Learning: PSet 3'
author: "Felipe Alamos"
date: "10/25/2019"
output: pdf_document
---

```{r, message=FALSE}
#Setups
library(dplyr)
library(skimr)
library(seriation)
library(ggplot2)
library(dbscan)
library(mixtools)
library(plotGMM)
library(gridExtra)
library(clValid)
```

#1.
_Load the state legislative professionalism data from the relevant subdirectory in this repo. See the codebook for reference in the same subdirectory and combine with our discussion of these data and the concept of state legislative professionalism from class for relevant background information._

```{r}
data <- get(load("StateLegProfData&Codebook/legprof-components.v1.0.RData"))
head(data)
```

#2.
_Munge the data:_
* select only the continuous features that should capture a state legislature's level of "professionalism" (session length (total and regular), salary, and expenditures); 
* restrict the data to only include the 2009/10 legislative session for consistency; 
* omit all missing values; 
* standardize the input features;
* and anything else you think necessary to get this subset of data into workable form (_hint_: consider storing the state names as a separate object to be used in plotting later)

```{r}
data_filtered <- data %>%
  filter(sessid == "2009/10") %>%
  na.omit() %>%
  select(t_slength, slength, salary_real, expend)

#Standardizing
data_scaled <- as.data.frame(scale(data_filtered))

states <- data %>%
  filter(sessid == "2009/10") %>%
  na.omit() %>%
  select(stateabv)
```


#3.
_Perform quick EDA visually or numerically and discuss the patterns you see._

```{r}
#skim(data_filtered), some problem with Knit happening
summary(data_filtered)
```

- Most of the data is from 2010
- All numerical variables are skewed to the left, i.e, they have significant outliers on their high end. This is particularly exacerbated for expenditure, where its max value is more than 100 times the median value. Lets have a closer look at expend:

```{r}
data_filtered_with_state<-data_filtered
data_filtered_with_state$state <- states$stateabv

ggplot(data_filtered_with_state, aes(x=reorder(as.factor(state),-expend), expend)) +
  geom_bar(stat="identity",color="black", fill="white")+
   theme(axis.text.x = element_text(size=9, angle = 90, vjust=-0.001))+
   labs(x = "States")
```

- We observe that the state with the extreme `expenditure` value is CA.
- We know try to understand correlation between the variables

#session length (total and regular), salary, and expenditures
```{r}
#stateabv, year, t_slength, slength, salary_real, expend
ggplot(data_scaled, aes(x=slength, y=salary_real)) +
  geom_point()
ggplot(data_scaled, aes(x=slength, y=expend)) +
  geom_point()
```

- We observe that there seems to be a positive correlation between length of regular session and salary. We might state the same for expenditure, but the correlation is much weaker.

# 4

_Diagnose clusterability in any way you'd prefer (e.g., sparse sampling, ODI, etc.); display the results and discuss the likelihood that natural, non-random structure exist in these data._

```{r}

df_dist<- dist(data_scaled)
dissplot(df_dist)
```

- From a quick look at this ODI, it seems that there is one big cluster, and then a group of points that are not necessarily clustered together.
- It does look that some outliers exist - elements that are very far away in distance from most others (this might be the ones grouping in the small cluster).

# 5
_Fit a k-means algorithm to these data and present the results. Give a quick, high level summary of the output and general patterns. Initialize the algorithm at k=2, and then check this assumption in the validation questions below._

```{r}
set.seed(123)
kmeans <- kmeans(data_scaled, 
              centers = 2,
              nstart = 15)

str(kmeans)

kmeans$size
```

- We observe two clusters, one with 43 elements and the other with 6.
- We present the centers of the two clusters:
```{r}
kmeans$centers
```
-And the list indicating to which cluster was each state associated
```{r}
kmeans$cluster

#We create a df where to save the clusters
data_scaled_with_clusters <- data_scaled
data_scaled_with_clusters$KmeansCluster <- as.factor(kmeans$cluster)
```

# 6.

_Fit a Gaussian mixture model via the EM algorithm to these data and present the results. Give a quick, high level summary of the output and general patterns. Initialize the algorithm at k=2, and then check this assumption in the validation questions below._

```{r}


#We use the multi-variate method so as to consider all columns of data
set.seed(123)
gmm1 <- mvnormalmixEM(data_scaled, k = 2) 
```

For the two cluster, we observe means on each of the variables:
```{r}
gmm1$mu
```
...standard deviations (notice that here we present the co variance matrix - in the diagonal we can find standard deviations)
```{r}
gmm1$sigma
```
... and heights:
```{r}
gmm1$lambda
```

... and the probabilities of each point being part of each of the two clusters:
```{r}
post <- as.data.frame(gmm1$posterior)

# quick viz distribution of mixture proportions
plot(post$comp.1, post$comp.2) 
```
- We can observe that states are represented either by one distribution or the other, not a mixture of them

```{r}
# get counts for each component 
post$component <- ifelse(post$comp.1 < 0.2, 2, 1)
table(post$component) 
```
- The first cluster has 44 states, the second one 5.

```{r}
# We add the clusters obtained by gmm to our dataframe
data_scaled_with_clusters$ClusterGMM <- post$component
```

# 7.
Fit one additional partitioning technique of your choice (e.g., PAM, CLARA, fuzzy C-means, DBSCAN, etc.), and present and discuss results. Here again initialize at k=2.

We will be using DBSCAN:
```{r}


#To identify the ideal epsilon, we can follow the techinque explained here: http://www.sthda.com/english/wiki/wiki.php?id_contents=7940 Nonetheless, we will use an epsilon that can generate 2 clusters, so as to compare it with the other algorithms.

dbscan <- dbscan(data_scaled, 
                    eps= 0.5) 
dbscan$cluster #0 indicates noise point

n_non_clustered <- sum(dbscan$cluster == 0)
n_clustered_in_1<- sum(dbscan$cluster == 1)
n_clustered_in_2<- sum(dbscan$cluster == 2)
```

-We observe that the 49 data points have been separated in two clusters. One with `r n_clustered_in_1` states, the other with `r n_clustered_in_2` states, and `r n_non_clustered` were not clustered.
-Its interesting to notice that this algorithm - because of its totally different approach to clustering compared to kmeans - allows for the detection of outliers or samples that are not significantly coherent members of any cluster. In addition, the algorithm gives us the chance to modify this through the parameter epsilon (which determines the distance that must exist between the points to be part of the same cluster)

```{r}
data_scaled_with_clusters$ClusterDBscan <- dbscan$cluster
```

# 8.

_Compare output of all in a visually useful, simple way (e.g., plotting by state cluster assignment across two features like salary and expenditures)._

```{r}
# View the salary by clusters
kmeans_salary_plot <- ggplot(data_scaled_with_clusters, aes(x = salary_real, y=expend)) + 
  geom_point(aes(colour = factor(KmeansCluster))) +
  labs(x = "Standarized salary",
       y = "Count of States",
       title = "KMeans Mixture Model") +
  scale_color_manual(values=c("chartreuse4", "dodgerblue4"),
                    name="Component") +
  theme_bw()

gmm_salary_plot <- ggplot(data_scaled_with_clusters, aes(x = salary_real, y=expend)) + 
  geom_point(aes(colour = factor(ClusterGMM))) +
  labs(x = "Standarized salary",
       y = "Count of States",
       title = "Gaussian Mixture Model") +
  scale_color_manual(values=c("chartreuse3", "dodgerblue3"),
                    name="Component") +
  theme_bw()

dbscan_salary_plot <- ggplot(data_scaled_with_clusters, aes(x = salary_real, y=expend)) + 
  geom_point(aes(colour = factor(ClusterDBscan))) +
  labs(x = "Standarized salary",
       y = "Count of States",
       title = "DBScan Model") +
  scale_color_manual(values=c("gainsboro", "chartreuse2", "dodgerblue1"),
                    name="Component") +
  theme_bw()

# view side by side
grid.arrange(kmeans_salary_plot, gmm_salary_plot, dbscan_salary_plot, ncol = 3)
```


- We can observe that KMeans and Gaussian Models fairly identify the same clusters. DBScan is more different and does not cluster many points, though some of them seem to be grouped in the same way as in the previous two methods.


# 9.

_Select a single validation strategy (e.g., compactness via min(WSS), average silhouette width, etc.), and calculate for all three algorithms. Display and compare your results for all three algorithms you fit (k-means, GMM, X)._ 

```{r}
#Possible values for clValid: "hierarchical", "kmeans", "diana", "fanny", "som", "model", "sota", "pam", "clara",
#I could not find a way to validate for gmm and dbscan, so doing it for hierarchical and pam instead

internal <- clValid(data_scaled, 2:10,
                    clMethods = c("kmeans", "pam", "hierarchical"),
                    validation = "internal")
summary(internal)

# par(mfrow = c(2, 2))
# plot(kmeans_internal, legend = FALSE,
#      type = "l",
#      main = " ")
```

Although the clValid method gives us scores for Connectivity, Dunn and Silhoutte, we will focus in a single validation strategy: Connectivity, due to its intuitive interpretation (measures proximity between individual observations in the same cluster, and hence a low connectivity score indicates good clustering).

We can observe that the hierarchical method is the one with best connectivity score, particularly when using 2 clusters (for 2 clusters, hierarchical has connectivity equal to 6.1, whereas kmeans and pam have 8.4 and 7.9 respectively). We can also notice that, for hierarchical, connectivity does not increase too much when moving from 2 to 3 clusters, but it does for kmeans and very significantly for pam.


# 10.

_Discuss the validation output._

* What can you take away from the fit?

First of all, interesting enough, we can realize how a pairwise technique (hierarchical) performs better than partitioning techniques. Nevertheless, the differences in performance are not that significant, which seem to validate both approaches
Second, its interesting to compare kmeans and pams, especially because they are fundamentally very similar in their algorithmic approach (only difference is in how to define the center of the clusters). Surprisingly, their performance on connectivity is very different, especially for k>2, with kmeans usually having better performance. This suggest that the idea of defining clusters based on fictitious points that are the result of averaging the points of a cluster, is better than using a point of the cluster to represent all of the set. 


* Which approach is optimal? And optimal at what value of k? 

When considering connectivity, the best approach is the hierarchical model, with k=2.
If we do not want to consider hierarchical in the analysis, the best approach is pam with k=2.

* What are reasons you could imagine selecting a technically "sub-optimal" partitioning method, regardless of the validation statistics? 

There might be domain specific restrictions that propose one method over other, or that makes some approach less reasonable. 
So, for example, it might be totally possible not to be willing to demand that all samples have to be in a cluster, and hence a gmm approach would be preferred, maybe relaxing restrictions of validations statistics.
It could also happen that grouping around a fictitious average of points (like kmeans does), is not appropriate for certain context, and hence we could choose pam. This is particularly useful in context where we want to inform the public which is this member representing the cluster. It could also be the case that features of the samples are discrete values, and hence using an "average" representation points makes less sense.
